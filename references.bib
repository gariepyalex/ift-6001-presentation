% Highly interesting
@article{Guo2017,
abstract = {Grasping has always been a great challenge for robots due to its lack of the ability to well understand the perceived sensing data. In this work, we propose an end-to-end deep vision network model to predict possible good grasps from real-world images in real time. In order to accelerate the speed of the grasp detection, reference rectangles are designed to suggest potential grasp locations and then refined to indicate robotic grasps in the image. With the pro-posed model, the graspable scores for each location in the image and the corresponding predicted grasp rectangles can be obtained in real time at a rate of 80 frames per second on a graphic processing unit. The model is evaluated on a real robot-collected data set and different reference rectangle settings are compared to yield the best detection perfor-mance. The experimental results demonstrate that the proposed approach can assist the robot to learn the graspable part of the object from the image in a fast manner.},
annote = {The authors apply a network architecture similar to Faster R-CNN (Ren et al., 2015) to grasping. This architecture provides a real-time detection at 80 FPS. Also, the article explores multiple performance metrics, being top N accuracy and different Intersection over Union (IoU) thresholds.},
author = {Guo, Di and Sun, Fuchun and Kong, Tao and Liu, Huaping},
doi = {10.1177/1729881416682706},
file = {:home/agariepy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guo et al. - Unknown - Deep vision networks for real-time robotic grasp detection.pdf:pdf},
journal = {International Journal of Advanced Robotic Systems},
keywords = {Robotic grasp detection,deep vision networks,reference rectangle},
title = {{Deep vision networks for real-time robotic grasp detection}},
url = {http://dx.doi.org/10.1177/1729881416682706{\%}0A},
volume = {14},
year = {2017}
}
@inproceedings{Jiang2011,
abstract = {Given an image and an aligned depth map of an object, our goal is to estimate the full 7-dimensional gripper configuration{\&}{\#}x2014;its 3D location, 3D orientation and the gripper opening width. Recently, learning algorithms have been successfully applied to grasp novel objects{\&}{\#}x2014;ones not seen by the robot before. While these approaches use low-dimensional representations such as a {\&}{\#}x2018;grasping point{\&}{\#}x2019; or a {\&}{\#}x2018;pair of points{\&}{\#}x2019; that are perhaps easier to learn, they only partly represent the gripper configuration and hence are sub-optimal. We propose to learn a new {\&}{\#}x2018;grasping rectangle{\&}{\#}x2019; representation: an oriented rectangle in the image plane. It takes into account the location, the orientation as well as the gripper opening width. However, inference with such a representation is computationally expensive. In this work, we present a two step process in which the first step prunes the search space efficiently using certain features that are fast to compute. For the remaining few cases, the second step uses advanced features to accurately select a good grasp. In our extensive experiments, we show that our robot successfully uses our algorithm to pick up a variety of novel objects.},
annote = {This paper proposes a new and simpler grasp point representation. Instead of using the classical 7 degrees of freedom approach, it proposes to instead represent a grasp point as a rotated rectangle, as seen from above the object. It has the advantages to be more computationally efficient and to be easier to annotate manually. In this optic, the paper also introduces a new dataset now known as the Cornell Grasping Dataset. The authors detect grasping rectangles using Histogram of Oriented Gradients (HOG) and Support Vector Machines (SVM).},
author = {Jiang, Yun and Moseson, Stephen and Saxena, Ashutosh},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2011.5980145},
file = {:home/agariepy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang, Moseson, Saxena - Unknown - Efficient Grasping from RGBD Images Learning using a new Rectangle Representation.pdf:pdf},
isbn = {9781612843865},
issn = {10504729},
pages = {3304--3311},
title = {{Efficient grasping from RGBD images: Learning using a new rectangle representation}},
url = {http://pr.cs.cornell.edu/grasping/jiang{\_}rectanglerepresentation{\_}fastgrasping.pdf},
year = {2011}
}
@inproceedings{Kappler2015,
abstract = {We propose a new large-scale database containing grasps that are applied to a large set of objects from numerous categories. These grasps are generated in simulation and are annotated with different grasp stability metrics. We use a descriptive and efficient representation of the local object shape at which each grasp is applied. Given this data, we present a two-fold analysis: (i) We use crowdsourcing to analyze the correlation of the metrics with grasp success as predicted by humans. The results show that the metric based on physics simulation is a more consistent predictor for grasp success than the standard {\&}{\#}x03C5;-metric. The results also support the hypothesis that human labels are not required for good ground truth grasp data. Instead the physics-metric can be used to generate datasets in simulation that may then be used to bootstrap learning in the real world. (ii) We apply a deep learning method and show that it can better leverage the large-scale database for prediction of grasp success compared to logistic regression. Furthermore, the results suggest that labels based on the physics-metric are less noisy than those from the {\&}{\#}x03C5;-metric and therefore lead to a better classification performance.},
annote = {The paper introduces a new database of simulated grasps. They compare using human-labeled grasp points and physics-based predictions as ground-truth when training a network. They conclude that physics-based predictions give better results and that human input is not required for learning grasping tasks.},
author = {Kappler, D and Bohg, J and Schaal, S},
booktitle = {Robotics and Automation (ICRA), 2015 IEEE International Conference on},
doi = {10.1109/ICRA.2015.7139793},
file = {:home/agariepy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kappler, Bohg, Schaal - 2015 - Leveraging big data for grasp planning.pdf:pdf},
keywords = {Big Data,Databases,Noise measurement,Robots,Shape,Stability analysis,Three-dimensional displays,control engineering computing,database management systems,grasp planning,grippers,large-scale database,learning method,logistic regression,physics simulation,physics-metric,planning (artificial intelligence)},
month = {05},
pages = {4304--4311},
publisher = {IEEE},
title = {{Leveraging big data for grasp planning}},
url = {http://ieeexplore.ieee.org/document/7139793/},
year = {2015}
}
@article{Kumra2016,
abstract = {Deep learning has significantly advanced computer vision and natural language processing. While there have been some successes in robotics using deep learning, it has not been widely adopted. In this paper, we present a novel robotic grasp detection system that predicts the best grasping pose of a parallel-plate robotic gripper for novel objects using the RGB-D image of the scene. The proposed model uses a deep convolutional neural network to extract features from the scene and then uses a shallow convolutional neural network to predict the grasp configuration for the object of interest. Our multi-modal model achieved an accuracy of 89.21{\%} on the standard Cornell Grasp Dataset and runs at real-time speeds. This redefines the state-of-the-art for robotic grasp detection.},
annote = {This paper propose a new multi-modal architecture based on the Residual Neural Network architecture. The network has two different inputs, being the RGB image and the depth image, which run on two parallel networks. Those are then merged before fully connected layers. It achieves state-of-the-art performance.},
archivePrefix = {arXiv},
arxivId = {1611.08036},
author = {Kumra, Sulabh and Kanan, Christopher},
eprint = {1611.08036},
file = {:home/agariepy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kumra, Kanan - 2016 - Robotic Grasp Detection using Deep Convolutional Neural Networks.pdf:pdf},
month = {11},
title = {{Robotic Grasp Detection using Deep Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1611.08036},
year = {2016}
}
@article{Lenz2015,
abstract = {We consider the problem of detecting robotic grasps in an RGB-D view of a scene containing objects. In this work, we apply a deep learning approach to solve this problem, which avoids time-consuming hand-design of features. This presents two main challenges. First, we need to evaluate a huge number of candidate grasps. In order to make detection fast and robust, we present a two-step cascaded system with two deep networks, where the top detections from the first are re-evaluated by the second. The first network has fewer features, is faster to run, and can effectively prune out unlikely candidate grasps. The second, with more features, is slower but has to run only on the top few detections. Second, we need to handle multimodal inputs effectively, for which we present a method that applies structured regularization on the weights based on multimodal group regularization. We show that our method improves performance on an RGBD robotic grasping dataset, and can be used to successfully execute grasps on two different robotic platforms.},
annote = {The authors introduce the use of deep learning for the grasp rectangle detection task. They propose a two step architecture. A first network suggests a large number of possible grasp locations on the image, then a second network scores the different locations to find the best one. Next, they implement the approach on two different robots, the Baxter and the PR2, and obtain respectively 84 and 89 percent success rate.},
archivePrefix = {arXiv},
arxivId = {1301.3592v5},
author = {Lenz, Ian and Lee, Honglak and Saxena, Ashutosh},
doi = {10.1177/0278364914549607},
eprint = {arXiv:1301.3592v5},
file = {:home/agariepy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lenz, Lee, Saxena - Unknown - Deep Learning for Detecting Robotic Grasps.pdf:pdf},
isbn = {9789810739379},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {3d feature learning,baxter,deep learning,pr2,rgb-d multi-modal data,robotic grasping},
number = {4-5},
pages = {705--724},
pmid = {19352402},
title = {{Deep learning for detecting robotic grasps}},
url = {http://arxiv.org/abs/1301.3592v5},
volume = {34},
year = {2015}
}
@article{Levine2016,
abstract = {We describe a learning-based approach to hand-eye coordination for robotic grasping from monocular images. To learn hand-eye coordination for grasping, we trained a large convolutional neural network to predict the probability that task-space motion of the gripper will result in successful grasps, using only monocular camera images and independently of camera calibration or the current robot pose. This requires the network to observe the spatial relationship between the gripper and objects in the scene, thus learning hand-eye coordination. We then use this network to servo the gripper in real time to achieve successful grasps. To train our network, we collected over 800,000 grasp attempts over the course of two months, using between 6 and 14 robotic manipulators at any given time, with differences in camera placement and hardware. Our experimental evaluation demonstrates that our method achieves effective real-time control, can successfully grasp novel objects, and corrects mistakes by continuous servoing.},
annote = {This paper proposes a different way to acquire grasping ground-truth data. Instead of using a manually annotated dataset, it proposes to gather data via trial-and-error of real robots over a long period of time, namely by using 14 robots over a two months period to acquire 800,000 successful grasps. Also, the paper shows that robots can learn coordination with a simple feed-forward CNN architecture, by opposition to other models that include time explicitly like LSTM.},
archivePrefix = {arXiv},
arxivId = {1603.02199},
author = {Levine, Sergey and Pastor, Peter and Krizhevsky, Alex and Quillen, Deirdre},
eprint = {1603.02199},
file = {:home/agariepy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Levine et al. - 2016 - Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection.pdf:pdf},
month = {03},
title = {{Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection}},
url = {http://arxiv.org/abs/1603.02199},
year = {2016}
}
@inproceedings{Pinto2016,
abstract = {Current learning-based robot grasping approaches exploit human-labeled datasets for training the models. However, there are two problems with such a methodology: (a) since each object can be grasped in multiple ways, manually labeling grasp locations is not a trivial task; (b) human labeling is biased by semantics. While there have been attempts to train robots using trial-and-error experiments, the amount of data used in such experiments remains substantially low and hence makes the learner prone to over-fitting. In this paper, we take the leap of increasing the available training data to 40 times more than prior work, leading to a dataset size of 50K data points collected over 700 hours of robot grasping attempts. This allows us to train a Convolutional Neural Network (CNN) for the task of predicting grasp locations without severe overfitting. In our formulation, we recast the regression problem to an 18-way binary classification over image patches. We also present a multi-stage learning approach where a CNN trained in one stage is used to collect hard negatives in subsequent stages. Our experiments clearly show the benefit of using large-scale datasets (and multi-stage training) for the task of grasping. We also compare to several baselines and show state-of-the-art performance on generalization to unseen objects for grasping.},
annote = {The authors present a way to gather a large grasping dataset via trial-and-error. They obtain a total of 50k positive grasps. The authors provide some insight on how such dataset can reduce severe overfitting compare to a smaller dataset, even when pretraining on ImageNet.},
archivePrefix = {arXiv},
arxivId = {1509.06825},
author = {Pinto, Lerrel and Gupta, Abhinav},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2016.7487517},
eprint = {1509.06825},
file = {:home/agariepy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pinto, Gupta - 2016 - Supersizing self-supervision Learning to grasp from 50K tries and 700 robot hours.pdf:pdf},
isbn = {9781467380263},
issn = {10504729},
pages = {3406--3413},
pmid = {25373136},
title = {{Supersizing self-supervision: Learning to grasp from 50K tries and 700 robot hours}},
volume = {2016-June},
url = {http://arxiv.org/abs/1509.06825},
year = {2016}
}
@INPROCEEDINGS{Redmon2015Grasp, 
author = {Redmon, Joseph and Angelova, Anelia},
booktitle={2015 IEEE International Conference on Robotics and Automation (ICRA)}, 
title={{Real-time grasp detection using convolutional neural networks}},
abstract = {— We present an accurate, real-time approach to robotic grasp detection based on convolutional neural networks. Our network performs single-stage regression to graspable bounding boxes without using standard sliding window or region proposal techniques. The model outperforms state-of-the-art approaches by 14 percentage points and runs at 13 frames per second on a GPU. Our network can simultaneously perform classification so that in a single step it recognizes the object and finds a good grasp rectangle. A modification to this model predicts multiple grasps per object by using a locally constrained prediction mechanism. The locally constrained model performs significantly better, especially on objects that can be grasped in a variety of ways.},
annote = {This article proposes three different network architectures to detect grasp rectangles, beating the state of the art at the time of publication by 14 percents. Another major contribution of the article is that for each of the three approaches, the grasp rectangles are computed in a single forward pass of a network. This means that the detection can run in real time on GPU, compared to 13.5 seconds per frame in previous work. Finally, the authors propose a way to use networks pretrained on ImageNet, a RGB dataset, to work with RGB-D data.},
year={2015}, 
pages={1316-1322}, 
keywords={image classification;neural nets;object detection;object recognition;regression analysis;robot vision;GPU;convolutional neural networks;grasp rectangle;graspable bounding boxes;locally constrained prediction mechanism;object recognition;real-time robotic grasp detection;single-stage regression;Accuracy;Computer architecture;Measurement;Predictive models;Robot kinematics;Training}, 
doi={10.1109/ICRA.2015.7139361}, 
ISSN={1050-4729}, 
month={05},}
@article{Rusu2016,
abstract = {Applying end-to-end learning to solve complex, interactive, pixel-driven control tasks on a robot is an unsolved problem. Deep Reinforcement Learning algorithms are too slow to achieve performance on a real robot, but their potential has been demonstrated in simulated environments. We propose using progressive networks to bridge the reality gap and transfer learned policies from simulation to the real world. The progressive net approach is a general framework that enables reuse of everything from low-level visual features to high-level policies for transfer to new tasks, enabling a compositional, yet simple, approach to building complex skills. We present an early demonstration of this approach with a number of experiments in the domain of robot manipulation that focus on bridging the reality gap. Unlike other proposed approaches, our real-world experiments demonstrate successful task learning from raw visual input on a fully actuated robot manipulator. Moreover, rather than relying on model-based trajectory optimisation, the task learning is accomplished using only deep reinforcement learning and sparse rewards.},
annote = {The authors propose a neural network architecture designed to transfer learned features from one task to another. More specifically, they demonstrate that the network can learn a task in simulation then transfer the learned task to a real robot. The authors contrast this approach with fine-tuning, which is a destructive approach consisting of replacing the last layers then retraining on a new task. Finally, the paper demonstrates the approach on experiments using the Jaco robotic arm.},
archivePrefix = {arXiv},
arxivId = {1610.04286},
url = {http://arxiv.org/abs/1610.04286},
author = {Rusu, Andrei A. and Vecerik, Matej and Roth{\"{o}}rl, Thomas and Heess, Nicolas and Pascanu, Razvan and Hadsell, Raia},
eprint = {1610.04286},
file = {:home/agariepy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rusu et al. - 2016 - Sim-to-Real Robot Learning from Pixels with Progressive Nets.pdf:pdf},
title = {{Sim-to-Real Robot Learning from Pixels with Progressive Nets}},
volume = {abs/1610.0},
year = {2016}
}
@inproceedings{Vahrenkamp2016,
abstract = {— In this work, we present a part-based grasp planning approach that is capable of generating grasps that are applicable to multiple familiar objects. We show how object models can be decomposed according to their shape and local volumetric information. The resulting object parts are labeled with semantic information and used for generating robotic grasping information. We investigate how the transfer of such grasping information to familiar objects can be achieved and how the transferability of grasps can be measured. We show that the grasp transferability measure provides valuable information about how successful planned grasps can be applied to novel object instances of the same object category. We evaluate the approach in simulation, by applying it to multiple object categories and determine how successful the planned grasps can be transferred to novel, but familiar objects. In addition, we present a use case on the humanoid robot ARMAR-III.},
annote = {They author approach grasping by transferring know grasps on templates, which are knows segments of 3D models, to real world familiar objects. This way, the robot can use semantic information, for instance knowing that it is grasping the handle of an object.},
author = {Vahrenkamp, Nikolaus and Westkamp, Leonard and Yamanobe, Natsuki and Aksoy, Eren E and Asfour, Tamim},
booktitle = {2016 IEEE-RAS International Conference on Humanoid Robots (Humanoids 2016)},
doi = {10.1109/HUMANOIDS.2016.7803382},
file = {:home/agariepy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vahrenkamp et al. - Unknown - Part-based Grasp Planning for Familiar Objects.pdf:pdf},
isbn = {9781509047178},
issn = {21640580},
title = {{Part-based Grasp Planning for Familiar Objects}},
year = {2016}
}
@article{Wang2016Multimodal,
abstract = {Autonomous manipulation has enabled a wide range of exciting robot tasks. However, perceiving outside environment is still a challenging problem in the field of intelligent robotic research due to the lack of object models, unstructured envir-onments, and time-consuming computation. In this article, we present a novel robot grasp detection system that maps a pair of RGB-D images of novel objects to best grasping pose of a robotic gripper. First, we segment the graspable objects from the unstructured scene using the geometrical features of both the object and the robotic gripper. Then, a deep convolutional neural network is applied on these graspable objects, which aims to find the best graspable area for each object. In order to improve the efficiency in the detection system, we introduce a structured penalty term to optimize the connections between multimodality, which significantly mitigates complexity of the network and outperforms fully connected multimodal processing. We also present a two-stage closed-loop grasping candidate estimator to accelerate the searching efficiency of grasping-candidate generation. Moreover, the combination of a two-stage estimator with the grasping detection network naturally improves detection accuracy. Experiments have been conducted to validate the proposed methods. The results show that our method outperforms the state of the art and runs at real-time speed.},
annote = {This paper proposes a deep learning approach for grasping that is different from what is usually seen in the literature because it is not a end-to-end approach. The author introduce a pipeline including hand-engineered object segmentation and a closed-loop grasp candidate refiner.},
author = {Wang, Zhichao and Li, Zhiqi and Wang, Bin and Liu, Hong},
doi = {10.1177/1687814016668077},
file = {:home/agariepy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2016 - Robot grasp detection using multimodal deep convolutional neural networks.pdf:pdf},
journal = {Special Issue Article Advances in Mechanical Engineering},
keywords = {RGB-D images,Robot grasp detection,deep convolutional neural network,multimodal information,two-stage closed-loop},
number = {9},
pages = {1--12},
title = {{Robot grasp detection using multimodal deep convolutional neural networks}},
volume = {8},
year = {2016}
}

% Interesting
@article{Bohg2013,
abstract = {—We review the work on data-driven grasp synthesis and the methodologies for sampling and ranking candidate grasps. We divide the approaches into three groups based on whether they synthesize grasps for known, familiar or unknown objects. This structure allows us to identify common object rep-resentations and perceptual processes that facilitate the employed data-driven grasp synthesis technique. In the case of known objects, we concentrate on the approaches that are based on object recognition and pose estimation. In the case of familiar objects, the techniques use some form of a similarity matching to a set of previously encountered objects. Finally, for the approaches dealing with unknown objects, the core part is the extraction of specific features that are indicative of good grasps. Our survey provides an overview of the different methodologies and discusses open problems in the area of robot grasping. We also draw a parallel to the classical approaches that rely on analytic formulations.},
annote = {Survey of different methods used in grasping, for both known and unknown objects. Focus on work done before the apparition of deep learning in the field.},
author = {Bohg, Jeannette and Morales, Antonio and Asfour, Tamim and Kragic, Danica},
doi = {10.1109/TRO.2013.2289018>},
file = {:home/agariepy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bohg et al. - Unknown - Data-Driven Grasp Synthesis -A Survey.pdf:pdf},
keywords = {Index Terms—Object grasping and manipulation,grasp planning,grasp syn-thesis,object recognition and classification,visual perception,visual representations},
title = {{Data-Driven Grasp Synthesis -A Survey}},
url = {http://arxiv.org/abs/1309.2660},
volume = {abs/1309.2},
year = {2013}
}
@article{Correll2016,
abstract = {This paper summarizes lessons learned from the first Amazon Picking Challenge in which 26 international teams designed robotic systems that competed to retrieve items from warehouse shelves. This task is currently performed by human workers, and there is hope that robots can someday help increase efficiency and throughput while lowering cost. We report on a 28-question survey posed to the teams to learn about each team's background, mechanism design, perception apparatus, planning and control approach. We identify trends in this data, correlate it with each team's success in the competition, and discuss observations and lessons learned.},
annote = {Results of a survey answered by the Amazon Picking Challenge participants. There are questions about techniques used for perceptions, software used, and many other questions.},
archivePrefix = {arXiv},
arxivId = {1601.05484},
author = {Correll, Nikolaus and Bekris, Kostas E. and Berenson, Dmitry and Brock, Oliver and Causo, Albert and Hauser, Kris and Okada, Kei and Rodriguez, Alberto and Romano, Joseph M. and Wurman, Peter R.},
doi = {10.1109/TASE.2016.2600527},
eprint = {1601.05484},
file = {:home/agariepy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Correll et al. - 2016 - Lessons from the Amazon Picking Challenge.pdf:pdf},
issn = {15455955},
journal = {IEEE Transactions on Automation Science and Engineering},
keywords = {Robot vision systems,manipulators,robotics,service robots,storage automation.},
month = {01},
number = {99},
title = {{Analysis and Observations from the First Amazon Picking Challenge}},
url = {http://arxiv.org/abs/1601.05484},
volume = {PP},
year = {2016}
}
@article{Jost2015,
abstract = {— Robust object recognition is a crucial ingredient of many, if not all, real-world robotics applications. This paper leverages recent progress on Convolutional Neural Networks (CNNs) and proposes a novel RGB-D architecture for object recognition. Our architecture is composed of two separate CNN processing streams – one for each modality – which are consecutively combined with a late fusion network. We focus on learning with imperfect sensor data, a typical problem in real-world robotics tasks. For accurate learning, we introduce a multi-stage training methodology and two crucial ingredients for handling depth data with CNNs. The first, an effective encoding of depth information for CNNs that enables learning without the need for large depth datasets. The second, a data augmentation scheme for robust learning with depth images by corrupting them with realistic noise patterns. We present state-of-the-art results on the RGB-D object dataset [15] and show recognition in challenging RGB-D real-world noisy settings.},
annote = {The paper focus on how to integrate the depth information of RGB-D images to CNNs for grasping tasks.},
archivePrefix = {arXiv},
arxivId = {1507.06821v2},
url = {http://arxiv.org/abs/1507.06821v2},
author = {Jost, Andreas Eitel and Springenberg, Tobias and Spinello, Luciano and Riedmiller, Martin and Burgard, Wolfram},
eprint = {arXiv:1507.06821v2},
file = {:home/agariepy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jost et al. - Unknown - Multimodal Deep Learning for Robust RGB-D Object Recognition.pdf:pdf},
title = {{Multimodal Deep Learning for Robust RGB-D Object Recognition}},
year = {2015}
}

@article{Redmon2016,
abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
annote = {State-of-the-art method for object detection on the MS COCO dataset. This task has many similarities with the grasping task.},
archivePrefix = {arXiv},
arxivId = {1612.08242},
author = {Redmon, Joseph and Farhadi, Ali},
eprint = {1612.08242},
file = {:home/agariepy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Redmon, Farhadi - Unknown - YOLO9000 Better, Faster, Stronger.pdf:pdf},
title = {{YOLO9000: Better, Faster, Stronger}},
url = {http://arxiv.org/abs/1612.08242},
year = {2016}
}
@article{Schwarz2017,
abstract = {— Part handling in warehouse automation is chal-lenging if a large variety of items must be accommodated and items are stored in unordered piles. To foster research in this domain, Amazon holds picking challenges. We present our system which achieved second and third place in the Amazon Picking Challenge 2016 tasks. The challenge required participants to pick a list of items from a shelf or to stow items into the shelf. Using two deep-learning approaches for object detection and semantic segmentation and one item model registration method, our system localizes the requested item. Manipulation occurs using suction on points determined heuristically or from 6D item model registration. Parametrized motion primitives are chained to generate motions. We present a full-system evaluation during the APC 2016 and component-level evaluations of the perception system on an annotated dataset.},
annote = {Deep learning method for object detection and semantic segmentation from dual RGB-D images. Grasping is done using a 6D suction gripper.},
author = {Schwarz, Max and Milan, Anton and Lenz, Christian and Mu, Aura and Periyasamy, Arul Selvam and Schreiber, Michael and Sch, Sebastian and Behnke, Sven},
file = {:home/agariepy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schwarz et al. - Unknown - NimbRo Picking Versatile Part Handling for Warehouse Automation.pdf:pdf},
journal = {IEEE International Conference on Robotics and Automation (ICRA)},
title = {{NimbRo Picking : Versatile Part Handling for Warehouse Automation}},
url = {http://www.milanton.de/files/icra2017/icra2017-schwarz.pdf},
year = {2017}
}
@inproceedings{Shafii2016,
annote = {This paper approaches grasping by template matching 3D point clouds acquired by a Kinect.},
author = {Shafii, Nima and Kasaei, S Hamidreza and Lopes, Luis Seabra},
booktitle = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
doi = {10.1109/IROS.2016.7759448},
file = {:home/agariepy/Downloads/IROS16{\_}1321{\_}FI.pdf:pdf},
keywords = {Education,Feature extraction,Grasping,Mahalanobis distance,Manipulators,Three-dimensional displays,Visualization,arm orientation,arm position,eature extraction,global visual features,grasp configuration,grasp demonstrations,grasp distance measure,grasp pose learning,grasp representation,grasp similarity metric,grasp template,image matching,intelligent robots,interactive incremental learning,interactive object view learning,learning systems,local visual features,manipulators,object grasping,object recognition,object view labels recognition,object view recognition,pose recognition,position control,robot vision,template matching},
pages = {2895--2900},
title = {{Learning to Grasp Familiar Objects using Object View Recognition and Template Matching}},
year = {2016}
}
@article{Veres2017,
abstract = {—Grasping is a complex process involving knowledge of the object, the surroundings, and of oneself. While humans are able to integrate and process all of the sensory information required for performing this task, equipping machines with this capability is an extremely challenging endeavor. In this paper, we investigate how deep learning techniques can allow us to translate high-level concepts such as motor imagery to the problem of robotic grasp synthesis. We explore a paradigm based on generative models for learning integrated object-action representations, and demonstrate its capacity for capturing and generating multimodal, multi-finger grasp configurations on a simulated grasping dataset.},
annote = {The authors use generative models on simulated data to create object representations that can be directly applied to grasp point generations.},
archivePrefix = {arXiv},
arxivId = {1701.03041},
author = {Veres, Matthew and Moussa, Medhat and Taylor, Graham W},
doi = {10.1109/LRA.2017.2651945},
eprint = {1701.03041},
file = {:home/agariepy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Veres, Moussa, Taylor - Unknown - Modeling Grasp Motor Imagery through Deep Conditional Generative Models.pdf:pdf},
issn = {2377-3766},
keywords = {Deep Learning,Generative Models,Index Terms—Grasping,Multifingered Hands,Visual Learning},
title = {{Modeling Grasp Motor Imagery through Deep Conditional Generative Models}},
url = {http://arxiv.org/abs/1701.03041},
year = {2017}
}
@inproceedings{Zaki2016,
annote = {This paper proposes a method to leverage RGB-D data on convolutional neural networks pretrained on only RGB data. This technique is applied to recognition tasks.},
author = {Zaki, Hasan F M and Shafait, Faisal and Mian, Ajmal},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2016.7487310},
file = {:home/agariepy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zaki, Shafait, Mian - 2016 - Convolutional hypercube pyramid for accurate RGB-D object category and instance recognition.pdf:pdf},
isbn = {9781467380263},
issn = {10504729},
pages = {1685--1692},
title = {{Convolutional hypercube pyramid for accurate RGB-D object category and instance recognition}},
volume = {2016-June},
year = {2016}
}
@article{Zeng2016AmazonPC,
abstract = {Robot warehouse automation has attracted significant interest in recent years, perhaps most visibly in the Amazon Picking Challenge (APC). A fully autonomous warehouse pick-and-place system requires robust vision that reliably recognizes and locates objects amid cluttered environments, self-occlusions, sensor noise, and a large variety of objects. In this paper we present an approach that leverages multi-view RGB-D data and self-supervised, data-driven learning to overcome those difficulties. The approach was part of the MIT-Princeton Team system that took 3rd- and 4th- place in the stowing and picking tasks, respectively at APC 2016. In the proposed approach, we segment and label multiple views of a scene with a fully convolutional neural network, and then fit pre-scanned 3D object models to the resulting segmentation to get the 6D object pose. Training a deep neural network for segmentation typically requires a large amount of training data. We propose a self-supervised method to generate a large labeled dataset without tedious manual segmentation. We demonstrate that our system can reliably estimate the 6D pose of objects under a variety of scenarios. All code, data, and benchmarks are available at http://www.andyzeng.com/apc2016.},
annote = {This paper proposes a system to segment, classify and find 6D bounding box of partially occluded objects from multiple RGB-D images.},
archivePrefix = {arXiv},
arxivId = {1609.09475},
author = {Zeng, Andy and Yu, Kuan-Ting and Song, Shuran and Suo, Daniel and Walker, Ed and Rodriguez, Alberto and Xiao, Jianxiong},
eprint = {1609.09475},
file = {:home/agariepy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zeng et al. - 2016 - Multi-view Self-supervised Deep Learning for 6D Pose Estimation in the Amazon Picking Challenge.pdf:pdf},
month = {09},
title = {{Multi-view Self-supervised Deep Learning for 6D Pose Estimation in the Amazon Picking Challenge}},
url = {http://arxiv.org/abs/1609.09475},
year = {2016}
}
@article{Shrivastava2016,
abstract = {With recent progress in graphics, it has become more tractable to train models on synthetic images, potentially avoiding the need for expensive annotations. However, learning from synthetic images may not achieve the desired performance due to a gap between synthetic and real image distributions. To reduce this gap, we propose Simulated+Unsupervised (S+U) learning, where the task is to learn a model to improve the realism of a simulator's output using unlabeled real data, while preserving the annotation information from the simulator. We develop a method for S+U learning that uses an adversarial network similar to Generative Adversarial Networks (GANs), but with synthetic images as inputs instead of random vectors. We make several key modifications to the standard GAN algorithm to preserve annotations, avoid artifacts and stabilize training: (i) a 'self-regularization' term, (ii) a local adversarial loss, and (iii) updating the discriminator using a history of refined images. We show that this enables generation of highly realistic images, which we demonstrate both qualitatively and with a user study. We quantitatively evaluate the generated images by training models for gaze estimation and hand pose estimation. We show a significant improvement over using synthetic images, and achieve state-of-the-art results on the MPIIGaze dataset without any labeled real data.},
annote = {Generative Adversarial Networks (GANs) to reduce the reality gap between simulated data and real data. Method applied to eye images and hand depth data.},
archivePrefix = {arXiv},
arxivId = {1612.07828},
author = {Shrivastava, Ashish and Pfister, Tomas and Tuzel, Oncel and Susskind, Josh and Wang, Wenda and Webb, Russ},
eprint = {1612.07828},
file = {:home/agariepy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shrivastava et al. - 2016 - Learning from Simulated and Unsupervised Images through Adversarial Training.pdf:pdf},
mendeley-groups = {ift6001/interesting},
month = {12},
title = {{Learning from Simulated and Unsupervised Images through Adversarial Training}},
url = {http://arxiv.org/abs/1612.07828},
year = {2016}
}


% Other
@inproceedings{Chebotar2016,
abstract = {? 2016 IEEE.We introduce a framework for learning regrasping behaviors based on tactile data. First, we present a grasp stability predictor that uses spatio-temporal tactile features collected from the early-object-lifting phase to predict the grasp outcome with a high accuracy. Next, the trained predictor is used to supervise and provide feedback to a reinforcement learning algorithm that learns the required grasp adjustments based on tactile feedback. Our results gathered over more than 50 hours of real robot experiments indicate that the robot is able to predict the grasp outcome with 93{\%} accuracy. In addition, the robot is able to improve the grasp success rate from 42{\%} when randomly grasping an object to up to 97{\%} when allowed to regrasp the object in case of a predicted failure.},
author = {Chebotar, Yevgen and Hausman, Karol and Su, Zhe and Sukhatme, G.S. and Schaal, Stefan},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2016.7759309},
file = {:home/agariepy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chebotar et al. - Unknown - Self-Supervised Regrasping using Spatio-Temporal Tactile Features and Reinforcement Learning.pdf:pdf},
isbn = {9781509037629},
issn = {21530866 21530858},
title = {{Self-supervised regrasping using spatio-temporal tactile features and reinforcement learning}},
volume = {2016-Novem},
year = {2016}
}
@article{Hossain2017,
author = {Hossain, Delowar and Capi, Genci and Jindai, Mitsuru},
doi = {10.1016/j.procs.2017.01.195},
file = {:home/agariepy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hossain, Capi, Jindai - 2017 - Evolution of Deep Belief Neural Network Parameters for Robot Object Recognition and Grasping.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
pages = {153--158},
title = {{Evolution of Deep Belief Neural Network Parameters for Robot Object Recognition and Grasping}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1877050917302132},
volume = {105},
year = {2017}
}
@article{Redmon2015YOLO,
abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
archivePrefix = {arXiv},
arxivId = {1506.02640},
author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
eprint = {1506.02640},
file = {:home/agariepy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Redmon et al. - 2015 - You Only Look Once Unified, Real-Time Object Detection(2).pdf:pdf},
mendeley-groups = {ift6001/other},
month = {06},
title = {{You Only Look Once: Unified, Real-Time Object Detection}},
url = {http://arxiv.org/abs/1506.02640},
year = {2015}
}
@article{Kanoulas2016,
author = {Kanoulas, Dimitrios and Lee, Jinoh and Caldwell, Darwin G and Tsagarakis, Nikos G},
file = {:home/agariepy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kanoulas et al. - 2016 - Visual Grasp Affordance Localization in Point Clouds using Curved Contact Patches.pdf:pdf},
issn = {0219-8436},
title = {{Visual Grasp Affordance Localization in Point Clouds using Curved Contact Patches}},
year = {2016}
}
@inproceedings{Mahler2016,
abstract = {— This paper presents Dexterity Network 1.0 (Dex-Net), a new dataset and associated algorithm to study the scaling effects of Big Data and Cloud Computation on robust grasp planning. The algorithm uses a Multi-Armed Bandit model with correlated rewards to leverage prior grasps and 3D object models in a growing dataset that currently includes over 10,000 unique 3D object models and 2.5 million parallel-jaw grasps. Each grasp includes an estimate of the probability of force closure under uncertainty in object and gripper pose and friction. Dex-Net 1.0 uses Multi-View Convolutional Neural Networks (MV-CNNs), a new deep learning method for 3D object classification, as a similarity metric between objects and the Google Cloud Platform to simultaneously run up to 1,500 virtual cores, reducing runtime by three orders of magnitude. Experiments suggest that using prior data can significantly benefit the quality and complexity of robust grasp planning. We report on system sensitivity to varying similarity metrics and pose and friction uncertainty levels. Code and additional in-formation can be found at: http://berkeleyautomation. github.io/dex-net/.},
author = {Mahler, Jeffrey and Pokorny, Florian T. and Hou, Brian and Roderick, Melrose and Laskey, Michael and Aubry, Mathieu and Kohlhoff, Kai and Kroger, Torsten and Kuffner, James and Goldberg, Ken},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2016.7487342},
file = {:home/agariepy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mahler et al. - 2016 - Dex-Net 1.0 A cloud-based network of 3D objects for robust grasp planning using a Multi-Armed Bandit model with c.pdf:pdf},
isbn = {9781467380263},
issn = {10504729},
pages = {1957--1964},
title = {{Dex-Net 1.0: A cloud-based network of 3D objects for robust grasp planning using a Multi-Armed Bandit model with correlated rewards}},
volume = {2016-June},
year = {2016}
}
@article{Mousavian2016,
abstract = {We present a method for 3D object detection and pose estimation from a single image. In contrast to current techniques that only regress the 3D orientation of an object, our method first regresses relatively stable 3D object properties using a deep convolutional neural network and then combines these estimates with geometric constraints provided by a 2D object bounding box to produce a complete 3D bounding box. The first network output estimates the 3D object orientation using a novel hybrid discrete-continuous loss, which significantly outperforms the L2 loss. The second output regresses the 3D object dimensions, which have relatively little variance compared to alternatives and can often be predicted for many object types. These estimates, combined with the geometric constraints on translation imposed by the 2D bounding box, enable us to recover a stable and accurate 3D object pose. We evaluate our method on the challenging KITTI object detection benchmark both on the official metric of 3D orientation estimation and also on the accuracy of the obtained 3D bounding boxes. Although conceptually simple, our method outperforms more complex and computationally expensive approaches that leverage semantic segmentation, instance level segmentation and flat ground priors and sub-category detection.},
archivePrefix = {arXiv},
arxivId = {1612.00496},
author = {Mousavian, Arsalan and Anguelov, Dragomir and Flynn, John and Kosecka, Jana},
eprint = {1612.00496},
file = {:home/agariepy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mousavian et al. - Unknown - 3D Bounding Box Estimation Using Deep Learning and Geometry.pdf:pdf},
title = {{3D Bounding Box Estimation Using Deep Learning and Geometry}},
url = {http://arxiv.org/abs/1612.00496v1},
year = {2016}
}
@inproceedings{Ottenhaus2016,
abstract = {— Autonomous grasping and manipulation of un-known objects is a central skill for humanoid robots. This is particularly challenging, as shape information needs to be obtained from sensory data which is often noisy and incomplete. However, object shape information is usually a key prerequisite for grasp and manipulation planning and thus needs to be estimated even if the available sensor data is limited. We propose a method for implicit surface modeling based on sparse contact information, as it arises e.g. from haptic exploration. Surfaces are locally defined using the contact points and their normals, and the object shape is extrapolated by integrating this partial information. For each contact contributing to the estimation, the local convexity or concavity is determined depending on its neighbors and their respective normals. Taking into account contact positions, normals and local convexities or concavities, the Implicit Shape Potential of the overall surface is generated. In contrast to popular methods based on Gaussian Processes, this representation allows for local details like edges and corners, without losing the ability to interpolate in the case of noise. In addition, it provides information to guide iterative exploration algorithms. The proposed method is evaluated on a set of various 3D shapes that possess flat and curved surface regions as well as convex and concave edges.},
archivePrefix = {arXiv},
arxivId = {1204.3968},
author = {Ottenhaus, Simon and Miller, Martin and Schiebener, David and Vahrenkamp, Nikolaus and Asfour, Tamim},
booktitle = {2016 IEEE-RAS International Conference on Humanoid Robots (Humanoids 2016)},
doi = {10.1109/HUMANOIDS.2016.7803372},
eprint = {1204.3968},
file = {:home/agariepy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ottenhaus et al. - Unknown - Local Implicit Surface Estimation for Haptic Exploration.pdf:pdf},
isbn = {9781509047178},
issn = {21640580},
pmid = {22255825},
title = {{Local Implicit Surface Estimation for Haptic Exploration}},
url = {http://arxiv.org/abs/1204.3968},
year = {2016}
}
@article{Patel2017,
abstract = {We describe the grasping and manipulation strategy that we employed at the autonomous track of the Robotic Grasping and Manipulation Competition at IROS 2016. A salient feature of our architecture is the tight coupling between visual (Asus Xtion) and tactile perception (Robotic Materials), to reduce the uncertainty in sensing and actuation. We demonstrate the importance of tactile sensing and reactive control during the final stages of grasping using a Kinova Robotic arm. The set of tools and algorithms for object grasping presented here have been integrated into the open-source Robot Operating System (ROS).},
archivePrefix = {arXiv},
arxivId = {1701.06071},
author = {Patel, Radhen and Cox, Rebecca and Romero, Branden and Correll, Nikolaus},
eprint = {1701.06071},
file = {:home/agariepy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Patel et al. - Unknown - Improving grasp performance using in-hand proximity and contact sensing.pdf:pdf},
title = {{Improving grasp performance using in-hand proximity and contact sensing}},
url = {http://arxiv.org/abs/1701.06071},
year = {2017}
}
@inproceedings{Puhlmann2016,
abstract = {— Observations of human grasping reveal that the exploitation of environmental constraints is a key structural aspect for the robustness and versatility of human grasping behavior. We analyze 3,400 human grasping trials with 17 sub-jects grasping 25 objects to show that viewing environmental constraints as the central structural aspect of human grasping yields surprisingly simple representations of human grasping behavior. We present hypothesis-driven experiments that em-phasize the centrality of environmental constraints in human grasping and extract from data a simple " grasping plan " that is a generative model for all of the human grasping trials we observed. This grasping plan can in principle be transferred to a robot system in an attempt to leverage environmental constraints to improve the performance of robotic grasping.},
author = {Puhlmann, Steffen and Heinemann, Fabian and Brock, Oliver and Maertens, Marianne},
booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2016.7759308},
file = {:home/agariepy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Puhlmann et al. - Unknown - A Compact Representation of Human Single-Object Grasping.pdf:pdf},
isbn = {9781509037612},
issn = {21530866},
title = {{A Compact Representation of Human Single-Object Grasping}},
year = {2016}
}
@article{Ren2015,
abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [7] and Fast R-CNN [5] have reduced the running time of these detection networks, exposing region pro-posal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolu-tional features. For the very deep VGG-16 model [18], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2{\%} mAP) and 2012 (70.4{\%} mAP) using 300 proposals per image. The code will be released.},
archivePrefix = {arXiv},
arxivId = {1506.01497v1},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
doi = {10.1016/j.nima.2015.05.028},
eprint = {arXiv:1506.01497v1},
file = {:home/agariepy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ren et al. - 2015 - Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks.pdf:pdf},
issn = {01689002},
journal = {Nips},
month = {06},
pages = {1--10},
pmid = {27295650},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
url = {http://arxiv.org/abs/1506.01497},
year = {2015}
}
@article{Rennie2016,
abstract = {An important logistics application of robotics involves manipulators that pick-and-place objects placed in warehouse shelves. A critical aspect of this task corre- sponds to detecting the pose of a known object in the shelf using visual data. Solving this problem can be assisted by the use of an RGB-D sensor, which also provides depth information beyond visual data. Nevertheless, it remains a challenging problem since multiple issues need to be addressed, such as low illumination inside shelves, clutter, texture-less and reflective objects as well as the limitations of depth sensors. This paper provides a new rich data set for advancing the state-of-the-art in RGBD- based 3D object pose estimation, which is focused on the challenges that arise when solving warehouse pick- and-place tasks. The publicly available data set includes thousands of images and corresponding ground truth data for the objects used during the first Amazon Picking Challenge at different poses and clutter conditions. Each image is accompanied with ground truth information to assist in the evaluation of algorithms for object detection. To show the utility of the data set, a recent algorithm for RGBD-based pose estimation is evaluated in this paper. Based on the measured performance of the algorithm on the data set, various modifications and improvements are applied to increase the accuracy of detection. These steps can be easily applied to a variety of different methodologies for object pose detection and improve performance in the domain of warehouse pick-and-place.},
archivePrefix = {arXiv},
arxivId = {1509.01277},
author = {Rennie, Colin and Shome, Rahul and Bekris, Kostas E and {De Souza}, Alberto F},
doi = {10.1109/LRA.2016.2532924},
eprint = {1509.01277},
file = {:home/agariepy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rennie et al. - 2016 - A Dataset for Improved RGBD-based Object Detection and Pose Estimation for Warehouse Pick-and-Place.pdf:pdf},
isbn = {9781467380256},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Manipulators,Manufacturing automation,Object detection,Object recognition,Robot vision systems},
number = {2},
pages = {1179--1185},
title = {{A Dataset for Improved RGBD-Based Object Detection and Pose Estimation for Warehouse Pick-and-Place}},
volume = {1},
url = {http://arxiv.org/abs/1509.01277},
year = {2016}
}
@inproceedings{Rocchi2016,
abstract = {— Compliant underactuated hands have been shown to be able to grasp a variety of objects while simplifying both mechanical and control complexity compared to fully-actuated hands. Building on recent advances in simulation software, this paper presents a generic underactuated compliant hand emulator working with state of the art simulation software that allows to evaluate the behavior of a compliant gripper grasping irregular objects. The emulator makes use of the adaptive synergy concept which can be generalizable to a wide array of underactuated compliant hands. The architecture of the simulator-emulator system is presented and the implementation details will be highlighted for two common compliant grippers, the Pisa/IIT SoftHand and the RightHand Robotics Reflex SF. These software tools are being used for the simulation track of the IROS 2016 Robot Grasping and Manipulation Competition in which teams are asked to develop novel controllers to grasp a variety of cluttered and irregular objects.},
author = {Rocchi, Alessio and Hauser, Kris},
booktitle = {2016 IEEE International Conference on Simulation, Modeling, and Programming for Autonomous Robots (SIMPAR)},
doi = {10.1109/SIMPAR.2016.7862352},
file = {:home/agariepy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rocchi, Hauser - Unknown - A Generic Simulator for Underactuated Compliant Hands.pdf:pdf},
keywords = {Actuators,Adaptation models,Grasping,Grippers,Mathematical model,Robot sensing systems},
pages = {37--42},
title = {{A Generic Simulator for Underactuated Compliant Hands}},
year = {2016}
}
@inproceedings{Wang2016SoftHand,
author = {Wang, Hong-Ying and Ling, Wing-Kuen},
booktitle = {Consumer Electronics-China (ICCE-China), 2016 IEEE International Conference on},
file = {:home/agariepy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - 07849757.pdf:pdf},
pages = {1--6},
title = {{Robotic grasp detection using deep learning and geometry model of soft hand}},
year = {2016}
}
@article{Zech2016,
abstract = {— We guess humans start acquiring grasping skills as early as at the infant stage by virtue of two key processes. First, infants attempt to learn grasps for known objects by imitating humans. Secondly, knowledge acquired during this process is reused in learning to grasp novel objects. We argue that these processes of active and transfer learning boil down to a random search of grasps on an object, suitably biased by prior experience. In this paper we introduce active learning of grasps for known objects as well as transfer learning of grasps for novel objects grounded on kernel adaptive, mode-hopping Markov Chain Monte Carlo. Our experiments show promising applicability of our proposed learning methods.},
archivePrefix = {arXiv},
arxivId = {1611.06368},
author = {Zech, Philipp and Piater, Justus},
eprint = {1611.06368},
file = {:home/agariepy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zech, Piater - Unknown - Active and Transfer Learning of Grasps by Sampling from Demonstration.pdf:pdf},
title = {{Active and Transfer Learning of Grasps by Sampling from Demonstration}},
url = {http://arxiv.org/abs/1611.06368},
year = {2016}
}
@article{Zeng20163DMatch,
abstract = {Matching local geometric features on real-world depth images is a challenging task due to the noisy, low-resolution, and incomplete nature of 3D scan data. These difficulties limit the performance of current state-of-art methods, which are typically based on histograms over ge-ometric properties. In this paper, we present 3DMatch, a data-driven model that learns a local volumetric patch de-scriptor for establishing correspondences between partial 3D data. To amass training data for our model, we propose an unsupervised feature learning method that leverages the millions of correspondence labels found in existing RGB-D reconstructions. Experiments show that our descriptor is not only able to match local geometry in new scenes for re-construction, but also generalize to different tasks and spa-tial scales (e.g. instance-level object model alignment for the Amazon Picking Challenge, and mesh surface corre-spondence). Results show that 3DMatch consistently out-performs other state-of-the-art approaches by a significant margin. Code, data, benchmarks, and pre-trained models are available online at http://3dmatch.cs.princeton.edu.},
author = {Zeng, Andy and Song, Shuran and Nie{\ss}ner, Matthias and Fisher, Matthew and Xiao, Jianxiong and Funkhouser, Thomas},
file = {:home/agariepy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zeng et al. - Unknown - 3DMatch Learning Local Geometric Descriptors from RGB-D Reconstructions.pdf:pdf},
title = {{3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions}},
archivePrefix = {arXiv},
arxivId = {1603.08182},
url = {http://arxiv.org/abs/1603.08182},
year = {2016}
}
@article{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
doi = {http://dx.doi.org/10.1016/j.protcy.2014.09.007},
eprint = {1102.0183},
file = {:home/agariepy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krizhevsky, Sutskever, Hinton - 2012 - ImageNet Classification with Deep Convolutional Neural Networks.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances In Neural Information Processing Systems},
mendeley-groups = {ift6001/other},
pages = {1--9},
pmid = {7491034},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks},
year = {2012}
}
@article{Szegedy2016,
abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at rel-atively low computational cost. Recently, the introduction of residual connections in conjunction with a more tradi-tional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Incep-tion networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These varia-tions improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We fur-ther demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08{\%} top-5 error on the test set of the ImageNet classification (CLS) challenge.},
archivePrefix = {arXiv},
arxivId = {1602.07261},
author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent},
eprint = {1602.07261},
file = {:home/agariepy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Szegedy et al. - 2016 - Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning(2).pdf:pdf},
journal = {Arxiv},
mendeley-groups = {ift6001/other},
month = {02},
pages = {12},
title = {{Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning}},
url = {http://arxiv.org/abs/1602.07261},
year = {2016}
}

@article{He2015,
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
eprint = {1512.03385},
file = {:home/agariepy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:pdf},
mendeley-groups = {ift6001/other},
title = {{Deep Residual Learning for Image Recognition}},
url = {https://arxiv.org/abs/1512.03385},
year = {2015}
}
@article{Deng2009,
abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ldquoImageNetrdquo, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
author = {Deng, Jia Deng Jia and Dong, Wei Dong Wei and Socher, Richard and Li, Li-Jia Li Li-Jia and Li, Kai Li Kai and Fei-Fei, Li Fei-Fei Li},
doi = {10.1109/CVPR.2009.5206848},
file = {:home/agariepy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Deng et al. - Unknown - ImageNet A Large-Scale Hierarchical Image Database.pdf:pdf},
isbn = {978-1-4244-3992-8},
issn = {1063-6919},
journal = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
mendeley-groups = {ift6001/other},
pages = {2--9},
pmid = {21914436},
title = {{ImageNet: A large-scale hierarchical image database}},
url = {http://www.eecs.umich.edu/eecs/about/articles/2016/05206848.pdf},
year = {2009}
}
@article{Kleinhans2015,
abstract = {Workshop on Robotic Hands, Grasping, and Manipulation, at the IEEE International Conference on Robotics and Automation, 26-30 May 2015, Seattle, Washington, USA},
author = {Kleinhans, A and Rosman, B and Michalik, M and Tripp, B and Detry, R},
file = {:home/agariepy/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kleinhans et al. - 2015 - G3DB A database of successful and failed grasps with RGB-D images, point clouds, mesh models and gripper param.pdf:pdf},
keywords = {Database,Grasping,Manipulation,Neurophysiology,Presentation},
mendeley-groups = {ift6001/interesting,ift6001/other},
publisher = {ICRA 2015},
title = {{G3DB: A database of successful and failed grasps with RGB-D images, point clouds, mesh models and gripper parameters}},
url = {http://researchspace.csir.co.za/dspace/handle/10204/8613},
year = {2015}
}
